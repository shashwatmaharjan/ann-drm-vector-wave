{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change fonts and specify font size\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "FONT_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "# Function to save files\n",
    "def save_file(values, file_name, file_directory):\n",
    "    \n",
    "    # Save the file as a .npy file\n",
    "    np.save(os.path.join(file_directory, file_name), values)\n",
    "    \n",
    "    print(f'Saved {file_name} to {file_directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Neural Network class\n",
    "class RESIDUAL():\n",
    "\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        # Initialize input_layer here\n",
    "        self.input_layer = None  \n",
    "\n",
    "    # Method to build the hidden layers\n",
    "    def build_hidden_layers(self):\n",
    "        \n",
    "        # Convolutional Layers\n",
    "        # First Convolutional Layer\n",
    "        x1 = tf.keras.layers.Conv1D(filters=200, kernel_size=32, padding='same', activation='LeakyReLU', kernel_initializer = 'glorot_normal')(self.input_layer)\n",
    "        x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        x2 = tf.keras.layers.Conv1D(filters=100, kernel_size=32, padding='same', activation='LeakyReLU', kernel_initializer = 'glorot_normal')(x1)\n",
    "        x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "\n",
    "        # Skip concatenated connection\n",
    "        x2 = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "        \n",
    "        # Third Convolutional Layer\n",
    "        x3 = tf.keras.layers.Conv1D(filters=400, kernel_size=32, padding='same', activation='LeakyReLU', kernel_initializer = 'glorot_normal')(x2)\n",
    "        x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "\n",
    "        # Skip concatenated connection\n",
    "        x3 = tf.keras.layers.Concatenate(axis=-1)([x2, x3])\n",
    "\n",
    "        return x3\n",
    "\n",
    "    # Method to build the overall model\n",
    "    def build_model(self):\n",
    "        \n",
    "        # Input layer\n",
    "        self.input_layer = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        # Hidden layer\n",
    "        hidden_layer = self.build_hidden_layers()\n",
    "\n",
    "        # Output Layer\n",
    "        output_layer = tf.keras.layers.Conv1D(filters=self.output_shape[1], kernel_size=32, padding='same', activation='LeakyReLU', kernel_initializer = 'glorot_normal')(hidden_layer)\n",
    "\n",
    "        # Build model\n",
    "        self.model = tf.keras.models.Model(inputs=[self.input_layer], outputs=[output_layer])\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # Method to compile the model\n",
    "    def compile(self, optimizer, loss, evaluation_metric):\n",
    "        \n",
    "        # Compile model\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=evaluation_metric)\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    # Define method to train the model\n",
    "    def train(self, x_train, y_train, epochs, batch_size, callbacks):\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks, validation_split=0.2)\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    # Method to print summary of model\n",
    "    def summary(self):\n",
    "        \n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plots:\n",
    "    \n",
    "    def __init__(self, history, file_directory):\n",
    "\n",
    "        self.history = history\n",
    "        self.file_directory = file_directory\n",
    "\n",
    "    def loss(self):\n",
    "\n",
    "        loss_name = list(self.history.history.keys())[0]\n",
    "\n",
    "        # Training\n",
    "        loss = self.history.history[loss_name]\n",
    "        val_loss = self.history.history['val_' + loss_name]\n",
    "\n",
    "        loss_plot = plt.figure()\n",
    "        epochs = range(1, len(loss)+1)\n",
    "        plt.plot(epochs, loss, 'bo--', label = 'Training Loss', markersize = 2)\n",
    "        plt.plot(epochs, val_loss, 'go--', label = 'Validation Loss', markersize = 2)\n",
    "        plt.title('Training and Validation Loss', fontsize=FONT_SIZE)\n",
    "        plt.xlabel('Epochs', fontsize=FONT_SIZE)\n",
    "        plt.ylabel('Loss', fontsize=FONT_SIZE)\n",
    "        plt.legend(['Training Loss', 'Validation Loss'], fontsize=FONT_SIZE)\n",
    "        ax = loss_plot.gca()\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.savefig(self.file_directory + '/loss.pdf', bbox_inches='tight')\n",
    "        \n",
    "        return loss_plot\n",
    "\n",
    "    def evaluation_metric(self):\n",
    "\n",
    "        metric_name = list(self.history.history.keys())[1]\n",
    "        \n",
    "        # Training\n",
    "        metric = self.history.history[metric_name]\n",
    "        val_metric = self.history.history['val_' + metric_name]\n",
    "\n",
    "        metric_plot = plt.figure()\n",
    "        epochs = range(1, len(metric)+1)\n",
    "        plt.plot(epochs, metric, 'bo--', label = 'Training Metric', markersize = 2)\n",
    "        plt.plot(epochs, val_metric, 'go--', label = 'Validation Metric', markersize = 2)\n",
    "        plt.title('Training and Validation Evaluation Metric', fontsize=FONT_SIZE)\n",
    "        plt.xlabel('Epochs', fontsize=FONT_SIZE)\n",
    "        plt.ylabel('Evaluation Metric', fontsize=FONT_SIZE)\n",
    "        plt.legend(['Training Metric', 'Validation Metric'], fontsize=FONT_SIZE)\n",
    "        ax = metric_plot.gca()\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        plt.savefig(self.file_directory + '/evaluation_metric.pdf', bbox_inches='tight')\n",
    "\n",
    "        return metric_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define directory for the normalized data\n",
    "normalized_data_directory = os.path.join(current_directory, '..', '..', 'data', 'normalized')\n",
    "\n",
    "# Define directory for the trained results\n",
    "trained_results_directory = os.path.join(current_directory, '..', '..', 'residual', 'training_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with just the displacement data to conserve memory\n",
    "# Load the normalized training subsets for displacement data\n",
    "print('Loading the normalized training subsets for displacement data...')\n",
    "normalized_training_displacement_data = np.load(os.path.join(normalized_data_directory, 'normalized_training_displacement_data.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normalized training subsets for force data\n",
    "print('Loading the normalized training subsets for force data...')\n",
    "normalized_training_force_data = np.load(os.path.join(normalized_data_directory, 'normalized_training_force_data.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the displacement and force data\n",
    "print(f'The shape of displacement data is {normalized_training_displacement_data.shape[1:]}.')\n",
    "print(f'The shape of force data is {normalized_training_force_data.shape[1:]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables that remain constant during the training\n",
    "input_shape = normalized_training_displacement_data.shape[1:]\n",
    "output_shape = normalized_training_force_data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Residual class\n",
    "model = RESIDUAL(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and the model\n",
    "model.build_model()\n",
    "model.compile(optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001), loss = 'mae', evaluation_metric = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "# Early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print('Training the model...')\n",
    "history = model.train(normalized_training_displacement_data, normalized_training_force_data, epochs = 500, batch_size = 32, callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plots(history, trained_results_directory)\n",
    "loss_plot = plot.loss()\n",
    "evaluation_metric_plot = plot.evaluation_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print('Saving the model...')\n",
    "model.model.save(os.path.join(trained_results_directory, 'model.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
