{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "from keras_tuner.engine.hyperparameters import HyperParameter as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "# Function to return searched optimizers\n",
    "def return_optimizer(optimizer_search, learning_rate_search):\n",
    "\n",
    "    # If else block to return the optimizer and learning rate\n",
    "    if optimizer_search == 'adam':\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_search)\n",
    "    \n",
    "    elif optimizer_search == 'nadam':\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate_search)\n",
    "    \n",
    "    elif optimizer_search == 'rmsprop':\n",
    "\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate_search)\n",
    "    \n",
    "    elif optimizer_search == 'adadelta':\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate_search)\n",
    "    \n",
    "    elif optimizer_search == 'adagrad':\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate_search)\n",
    "\n",
    "    elif optimizer_search == 'adamax':\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate_search)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save files\n",
    "def save_file(values, file_name, file_directory):\n",
    "    \n",
    "    # Save the file as a .npy file\n",
    "    np.save(os.path.join(file_directory, file_name), values)\n",
    "    \n",
    "    print(f'Saved {file_name} to {file_directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build model for hyperparameter search\n",
    "def build_model(hp):\n",
    "\n",
    "    # Model Architecture Stage\n",
    "    # Filter Sizes\n",
    "    filter_search = hp.Int('num_of_filters', min_value = 100, max_value = 1000, step = 100)\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional Layers\n",
    "    # First Convolutional Layer\n",
    "    x1 = tf.keras.layers.Conv1D(filters=filter_search,\n",
    "                                kernel_size=32,\n",
    "                                padding='same',\n",
    "                                kernel_initializer='glorot_normal',\n",
    "                                activation='LeakyReLU')(input_layer)\n",
    "    \n",
    "    # Batch Normalization Layer\n",
    "    x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "    # Define the skip connection\n",
    "    skip_connection_1 = x1\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    x2 = tf.keras.layers.Conv1D(filters=filter_search,\n",
    "                                kernel_size=32,\n",
    "                                padding='same',\n",
    "                                kernel_initializer='glorot_normal',\n",
    "                                activation='LeakyReLU')(x1)\n",
    "    \n",
    "    # Batch Normalization Layer\n",
    "    x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "\n",
    "    # Add the skip connection connection\n",
    "    x2 = tf.keras.layers.Add()([skip_connection_1, x2])\n",
    "\n",
    "    # Define the skip connection\n",
    "    skip_connection_2 = x2\n",
    "    \n",
    "    # Third Convolutional Layer\n",
    "    x3 = tf.keras.layers.Conv1D(filters=filter_search,\n",
    "                                kernel_size=32,\n",
    "                                padding='same',\n",
    "                                kernel_initializer='glorot_normal',\n",
    "                                activation='LeakyReLU')(x2)\n",
    "    \n",
    "    # Batch Normalization Layer\n",
    "    x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "\n",
    "    # Add the skip connection connection\n",
    "    x3 = tf.keras.layers.Add()([skip_connection_2, x3])\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer = tf.keras.layers.Conv1D(filters=output_shape[1], kernel_size=32, padding='same', activation='LeakyReLU')(x3)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.models.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001), \n",
    "                  loss = 'mae', \n",
    "                  metrics = 'mse')\n",
    "    \n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define directory for the normalized data\n",
    "normalized_data_directory = os.path.join(current_directory, '..', '..', 'data', 'normalized')\n",
    "\n",
    "# Define directory for the hyperparameter search\n",
    "save_directory = os.path.join(current_directory, '..', '..', 'residual-concatenate')\n",
    "hyperparameter_search_folder_name = 'hyperparameter_search_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with just the displacement data to conserve memory\n",
    "# Load the normalized training subsets for displacement data\n",
    "print('Loading the normalized training subsets for displacement data...')\n",
    "normalized_training_displacement_data = np.load(os.path.join(normalized_data_directory, 'normalized_training_displacement_data.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normalized training subsets for force data\n",
    "print('Loading the normalized training subsets for force data...')\n",
    "normalized_training_force_data = np.load(os.path.join(normalized_data_directory, 'normalized_training_force_data.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the displacement data\n",
    "print(f'The shape of displacement data is {normalized_training_displacement_data.shape[1:]}.')\n",
    "print(f'The shape of force data is {normalized_training_force_data.shape[1:]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables that remain constant during the training\n",
    "input_shape = normalized_training_displacement_data.shape[1:]\n",
    "output_shape = normalized_training_force_data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the hyperband tuner from keras tuner\n",
    "hyperband_tuner = kt.Hyperband(build_model,\n",
    "                                objective = kt.Objective('val_mse', direction = 'min'),\n",
    "                                max_epochs = 500,\n",
    "                                directory = save_directory,\n",
    "                                project_name = 'hyperband_search_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "# Early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display search space summary\n",
    "hyperband_tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80-20 into training and validation sets\n",
    "normalized_training_displacement_data = normalized_training_displacement_data[:int(0.8 * normalized_training_displacement_data.shape[0])]\n",
    "normalized_training_force_data = normalized_training_force_data[:int(0.8 * normalized_training_force_data.shape[0])]\n",
    "\n",
    "normalized_validation_displacement_data = normalized_training_displacement_data[int(0.8 * normalized_training_displacement_data.shape[0]):]\n",
    "normalized_validation_force_data = normalized_training_force_data[int(0.8 * normalized_training_force_data.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best hyperparameters\n",
    "hyperband_tuner.search(normalized_training_displacement_data, normalized_training_force_data,\n",
    "                        epochs = 100,\n",
    "                        validation_data = (normalized_validation_displacement_data, normalized_validation_force_data),\n",
    "                        callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best trained model\n",
    "best_model = hyperband_tuner.get_best_models(num_models = 1)[0]\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(os.path.join(save_directory, 'training_results', 'best_model.h5'))\n",
    "\n",
    "# Print message to the user\n",
    "print('Hyperparameter search completed successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
